{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc8a4d8-e682-4765-b140-277eaa795ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcindabrowski/conda/envs/3.11-research/lib/python3.11/site-packages/quantining/base/engine.py:15: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import quantining.base as qt\n",
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dad5633-a008-453e-9a77-ce1faeea73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Literal\n",
    "\n",
    "class BracketAccess(type):\n",
    "    def __getitem__(cls, key: str):\n",
    "        return getattr(cls, key, None)\n",
    "\n",
    "class SampleMethods(metaclass=BracketAccess):\n",
    "\n",
    "    @staticmethod\n",
    "    def noise(scale: float, num_time_steps: int, **kwargs) -> np.ndarray:\n",
    "        return np.random.normal(scale=scale, size=num_time_steps)\n",
    "\n",
    "    @staticmethod\n",
    "    def brownian_motion(num_time_steps: int, initial_value: int, drift=0.0, \n",
    "                                volatility=1.0, \n",
    "                                dt=1.0, **kwargs):\n",
    "                                    \n",
    "        increments = np.random.normal(loc=drift*dt, scale=volatility*np.sqrt(dt), size=num_time_steps)\n",
    "    \n",
    "        # Generate forward Brownian motion path\n",
    "        path = np.cumsum(increments) + initial_value\n",
    "    \n",
    "        return path\n",
    "\n",
    "    @staticmethod\n",
    "    def random_oscillator(uniform_range: Tuple[int], num_time_steps: int, **kwargs) -> np.ndarray:\n",
    "        return np.cos(np.random.uniform(*uniform_range, num_time_steps))\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize(input_array: np.ndarray) -> np.ndarray:\n",
    "        return (input_array - input_array.mean())/input_array.std()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(input_array: np.ndarray) -> np.ndarray:\n",
    "        return (input_array - input_array.min())/(input_array.max()-input_array.min())\n",
    "\n",
    "    @staticmethod\n",
    "    def random_dataset(n_series: int, \n",
    "                       series_types: List[Literal['noise', 'brownian_motion', 'random_oscillator']], \n",
    "                       **kwargs) -> pd.DataFrame:\n",
    "\n",
    "        assert len(series_types) == n_series, AssertionError('len(series_types) must be equal to n_series')\n",
    "        for name in series_types:\n",
    "            assert name in ['noise', 'brownian_motion', 'random_oscillator'], AssertionError(f\"{name} must be one of ['noise', 'brownian_motion', 'ranom_oscillator']\")\n",
    "        \n",
    "        dataset = {}\n",
    "                           \n",
    "        for a, name in enumerate(series_types):\n",
    "            data = SampleMethods[name](**kwargs)\n",
    "\n",
    "            if 'transform' in kwargs.keys():\n",
    "                match kwargs['transform']:\n",
    "                    case 'normalize':\n",
    "                        data = SampleMethods.normalize(data)\n",
    "                    case 'standardize':\n",
    "                        data = SampleMethods.standardize(data)\n",
    "                    case other:\n",
    "                        pass\n",
    "                        \n",
    "            dataset[f\"{name}_{a}\"] = data\n",
    "\n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "    @staticmethod\n",
    "    def all():\n",
    "        return [key for key in SampleMethods.__dict__.keys() if not key.startswith('_')][:-1]\n",
    "\n",
    "                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0b8ee24-69e4-4ec5-bd52-475b10654057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowTransform(metaclass=BracketAccess):\n",
    "\n",
    "    @staticmethod\n",
    "    def sliding_window(df: pd.DataFrame, window_length: int):\n",
    "        sw = np.squeeze(np.lib.stride_tricks.sliding_window_view(df.values, (window_length,df.shape[-1])))\n",
    "        return np.swapaxes(sw, 1,-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def non_overlapping_window(df: pd.DataFrame, window_length: int):\n",
    "        idxs = np.arange(df.shape[0]//window_length) * window_length\n",
    "        stacked = np.dstack([df.iloc[idx:idx+window_length].values for idx in idxs])\n",
    "        return np.swapaxes(stacked, 0,-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def all():\n",
    "        return [key for key in WindowTransform.__dict__.keys() if not key.startswith('_')][:-1]\n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40b98738-7711-4bcb-a4c5-12470e69cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c397a128-b327-4a58-9014-db108b4b4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MultivariateDataset(Dataset):\n",
    "    \n",
    "    def __new__(cls, \n",
    "                mode: Literal['sliding_window', 'non_overlapping_window'], \n",
    "                df: pd.DataFrame,\n",
    "                window_length: int, \n",
    "                transform=None):\n",
    "        assert mode in WindowTransform.all(), AssertionError(f'mode must be on of {WindowTransform.methods()}')\n",
    "\n",
    "        return super().__new__(cls)\n",
    "        \n",
    "    def __init__(self, \n",
    "                 mode: Literal['sliding_window', 'non_overlapping_window'], \n",
    "                 df: pd.DataFrame, \n",
    "                 window_length: int, \n",
    "                 transform=None):\n",
    "                     \n",
    "        global DEVICE\n",
    "                     \n",
    "        self.tensors = torch.FloatTensor(WindowTransform[mode](df, window_length)).to(DEVICE)  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.tensors[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transofrm(sample)\n",
    "\n",
    "        return sample.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "85463067-a17c-4f0b-95aa-1d392d5b2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = SampleMethods.random_dataset(9, ['random_oscillator','random_oscillator','random_oscillator','brownian_motion','brownian_motion','brownian_motion','noise','noise','noise'], uniform_range=(0,1), num_time_steps=830*2880, initial_value=100, scale=0.5, transform='standardize')\n",
    "SAMPLE_DATASET = MultivariateDataset('non_overlapping_window', DF, 2880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "d90877b6-586c-4dbe-9027-473f59ff15b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([830, 9, 2880])"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_DATASET.tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "4ba2ff61-c9f0-4b9d-a880-92c1db64b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.transformer import MultiheadAttention\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, conv_filters, conv_kernel_size, conv_strides,\n",
    "                 attention_heads, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Initialize lists for convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Add convolutional layers\n",
    "        for i in range(len(conv_filters)):\n",
    "            self.add_conv_layer(input_dim if i==0 else conv_filters[i-1],\n",
    "                                conv_filters[i], conv_kernel_size[i], conv_strides[i])\n",
    "\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.self_attention = MultiheadAttention(embed_dim=conv_filters[-1], num_heads=attention_heads)\n",
    "\n",
    "        # Fully connected layers to output the mean and standard deviation vectors\n",
    "        self.fc_mu = nn.Linear(conv_filters[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(conv_filters[-1], latent_dim)\n",
    "\n",
    "\n",
    "    def add_conv_layer(self, in_channels, out_channels, kernel_size, stride):\n",
    "        # Function to add a Convolutional layer followed by a ReLU activation\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, stride))\n",
    "        self.conv_layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass through each Convolutional layer\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Store the output shape of the last Convolutional layer\n",
    "        self.last_conv_output_shape = x.shape\n",
    "\n",
    "        # Reshape x to match what the multi-head attention layer expects\n",
    "        x = x.permute(2, 0, 1)  # shape becomes (L, N, E)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x, _ = self.self_attention(x, x, x)\n",
    "\n",
    "        # Fully connected layers to output the mean and standard deviation vectors\n",
    "        x = x.mean(dim=0)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Function to generate a random sample from the distribution defined by mu and logvar\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, conv_transpose_filters, conv_transpose_kernel_sizes, conv_transpose_strides, upsample):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Upsample layer\n",
    "        self.upsample = nn.Upsample(scale_factor=upsample)  # adjust this value as needed\n",
    "\n",
    "        # Initialize list for Convolutional Transpose layers\n",
    "        self.conv_transpose_layers = nn.ModuleList()\n",
    "\n",
    "        # Add Convolutional Transpose layers\n",
    "        for i in range(len(conv_transpose_filters)):\n",
    "            self.add_conv_transpose_layer(hidden_dim if i==0 else conv_transpose_filters[i-1],\n",
    "                                          conv_transpose_filters[i], conv_transpose_kernel_sizes[i], conv_transpose_strides[i])\n",
    "\n",
    "    def add_conv_transpose_layer(self, in_channels, out_channels, kernel_size, stride):\n",
    "        # Function to add a Convolutional Transpose layer followed by a ReLU activation\n",
    "        self.conv_transpose_layers.append(nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride))\n",
    "        self.conv_transpose_layers.append(nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Fully connected layer\n",
    "        z = F.relu(self.fc(z))\n",
    "\n",
    "        # Reshape to 3D tensor \n",
    "        z = z.view(-1, self.hidden_dim, 1)\n",
    "\n",
    "        # Upsample\n",
    "        z = self.upsample(z)\n",
    "\n",
    "        # Pass through each Convolutional Transpose layer\n",
    "        for layer in self.conv_transpose_layers:\n",
    "            z = layer(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "class HybridVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Inspired by... with some additional changes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim, encoder_params, decoder_params):\n",
    "        super(HybridVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, *encoder_params, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, *decoder_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.encoder.reparameterize(mu, logvar)\n",
    "        \n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "class Metrics(metaclass=BracketAccess):\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y_true, y_pred, **kwargs):\n",
    "        return torch.mean(torch.abs(y_true-y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred, **kwargs):\n",
    "        return torch.mean((y_true-y_pred)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mase(y_true, y_pred, y_naive, **kwargs):\n",
    "        mae = Metrics.mae(y_true, y_pred)\n",
    "        scale = Metrics.mae(y_true, y_naive)\n",
    "        return mae/scale\n",
    "\n",
    "    def all():\n",
    "        return [key for key in Metrics.__dict__.keys() if not key.startswith(\"_\")][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "a4d725cd-2020-4eb6-9dc6-9e73f7fff6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "Optimizers = Enum(\"Optimizers\", {\n",
    " 'Adadelta': torch.optim.Adadelta,\n",
    " 'Adagrad': torch.optim.Adagrad,\n",
    " 'Adam': torch.optim.Adam,\n",
    " 'AdamW': torch.optim.AdamW,\n",
    " 'SparseAdam': torch.optim.SparseAdam,\n",
    " 'Adamax': torch.optim.Adamax,\n",
    " 'ASGD': torch.optim.ASGD,\n",
    " 'SGD': torch.optim.SGD,\n",
    " 'RAdam': torch.optim.RAdam,\n",
    " 'Rprop': torch.optim.Rprop,\n",
    " 'RMSprop': torch.optim.RMSprop,\n",
    " 'Optimizer': torch.optim.Optimizer,\n",
    " 'NAdam': torch.optim.NAdam,\n",
    " 'LBFGS': torch.optim.LBFGS,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "123c740f-f413-4d43-84e4-3d8667c50155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Sequence\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "class Reducer:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __new__(cls, \n",
    "                dataset: MultivariateDataset, \n",
    "                batch_size: int,\n",
    "                optimizer: Literal['Adadelta','Adagrad','Adam','AdamW','SparseAdam','Adamax','ASGD','SGD','RAdam','Rprop','RMSprop','Optimizer','NAdam','LBFGS'],\n",
    "                latent_dim: int, \n",
    "                conv_filters: Sequence[int], \n",
    "                conv_kernel_size: Sequence[int], \n",
    "                conv_strides: Sequence[int], \n",
    "                attention_heads: int,\n",
    "                hidden_dim: int, \n",
    "                conv_transpose_filters: Sequence[int], \n",
    "                conv_transpose_kernel_sizes:Sequence[int], \n",
    "                conv_transpose_strides:Sequence[int], \n",
    "                upsample: int):\n",
    "\n",
    "        \n",
    "        assert len(conv_filters) == len(conv_kernel_size) == len(conv_strides), AssertionError(\"All encoder arguments have to have same length\")\n",
    "        assert isinstance(dataset, MultivariateDataset), AssertionError(\"Dataset have to be of type MultivariateDataset\")\n",
    "\n",
    "        return super().__new__(cls)\n",
    "                     \n",
    "    def __init__(self, \n",
    "                 dataset: MultivariateDataset, \n",
    "                 batch_size: int,\n",
    "                 optimizer: Literal['Adadelta','Adagrad','Adam','AdamW','SparseAdam','Adamax','ASGD','SGD','RAdam','Rprop','RMSprop','Optimizer','NAdam','LBFGS'],\n",
    "                 latent_dim: int, \n",
    "                 conv_filters: Sequence[int], \n",
    "                 conv_kernel_size: Sequence[int], \n",
    "                 conv_strides: Sequence[int], \n",
    "                 attention_heads: int,\n",
    "                 hidden_dim: int, \n",
    "                 conv_transpose_filters: Sequence[int], \n",
    "                 conv_transpose_kernel_sizes:Sequence[int], \n",
    "                 conv_transpose_strides:Sequence[int], \n",
    "                 upsample: int) -> None:\n",
    "    \n",
    "        global DEVICE\n",
    "\n",
    "        self.data_loader = DataLoader(dataset, batch_size=batch_size)                      \n",
    "        self.model = HybridVAE(input_dim=dataset.tensors.shape[1], \n",
    "                               latent_dim=latent_dim,\n",
    "                               encoder_params=(conv_filters, conv_kernel_size, conv_strides, attention_heads),\n",
    "                               decoder_params=(hidden_dim, conv_transpose_filters, \n",
    "                                               conv_transpose_kernel_sizes, conv_transpose_strides, \n",
    "                                               upsample)).to(DEVICE)\n",
    "        self.optimizer = Optimizers[optimizer].value(params=self.model.parameters())\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_hybrid_vae(recon_x, x, mu, logvar):\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(recon_x, x)\n",
    "    \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "        # Total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "    \n",
    "        return loss\n",
    "        \n",
    "    def fit(self,\n",
    "            epochs: int = 5, \n",
    "            metrics: Sequence[str] = None, schedule: bool = False,\n",
    "            **kwargs) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Kwargs y_naive if metrics is mase\n",
    "        \"\"\"\n",
    "        \n",
    "        if metrics != None:\n",
    "            assert hasattr(metrics, \"__iter__\"), AssertionError('If not none, metrics have to be iterable object')\n",
    "            for name in metrics:\n",
    "                assert name in Metrics.all(), AssertionError(f\"{name} must be one of {Metrics.all()}.\")\n",
    "        \n",
    "    \n",
    "        desc = \"Fitting VAE model on dataset...\"\n",
    "                \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "        #schedule = kwargs['schedule'] if 'schedule' in kwargs.keys() else None\n",
    "                \n",
    "        # Training loop\n",
    "        for epoch in tqdm(range(epochs), total=epochs, desc=desc):\n",
    "            summary = {'epoch':epoch+1}\n",
    "            for batch in self.data_loader:\n",
    "                x = batch[0]\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                loss = Reducer.loss_hybrid_vae(recon_x, x, mu, logvar)\n",
    "                summary['loss'] = loss.item() #.:4f\n",
    "                if metrics is not None:\n",
    "                    for name in metrics:\n",
    "                        summary[name] = Metrics[name](recon_x, x, **kwargs).item() #:.4f\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            print(summary)\n",
    "            \n",
    "            if schedule:\n",
    "                scheduler.step(loss)\n",
    "\n",
    "    def generate(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def latent_rep(self, **kwargs) -> np.ndarray:\n",
    "\n",
    "        encoded = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.data_loader):\n",
    "                x = batch[0]\n",
    "                mu, logvar = self.model.encoder(x)\n",
    "                z = self.model.encoder.reparameterize(mu, logvar)\n",
    "                encoded.append(z)\n",
    "\n",
    "        return np.vstack([item.cpu().numpy() for item in encoded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "e5810d6c-7921-4ad6-8df1-ccf878734bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_FILTERS = [64,128]\n",
    "EXAMPLE_PARAMS = {'dataset': SAMPLE_DATASET, \n",
    "                 'batch_size': 32,\n",
    "                 'optimizer': 'Adam',\n",
    "                 'latent_dim': 1000, \n",
    "                 'conv_filters': CONV_FILTERS,\n",
    "                 'conv_kernel_size': [3,3], \n",
    "                 'conv_strides': [2,2],\n",
    "                 'attention_heads':8,\n",
    "                 'hidden_dim': 400, \n",
    "                 'conv_transpose_filters': [128,9], \n",
    "                 'conv_transpose_kernel_sizes': [2,2], \n",
    "                 'conv_transpose_strides': [2,2], \n",
    "                 'upsample': SAMPLE_DATASET.tensors.shape[-1]/(len(CONV_FILTERS)*2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "f9019863-63f1-48cd-b1a8-6e26b8a9399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720.0 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "r = Reducer(**EXAMPLE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "5a522859-530c-4e79-bb9e-fa562e0f5885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HybridVAE(\n",
       "  (encoder): Encoder(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): Conv1d(9, 64, kernel_size=(3,), stride=(2,))\n",
       "      (1): ReLU()\n",
       "      (2): Conv1d(64, 128, kernel_size=(3,), stride=(2,))\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (self_attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (fc_mu): Linear(in_features=128, out_features=1000, bias=True)\n",
       "    (fc_logvar): Linear(in_features=128, out_features=1000, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (fc): Linear(in_features=1000, out_features=400, bias=True)\n",
       "    (upsample): Upsample(scale_factor=720.0, mode='nearest')\n",
       "    (conv_transpose_layers): ModuleList(\n",
       "      (0): ConvTranspose1d(400, 128, kernel_size=(2,), stride=(2,))\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): ConvTranspose1d(128, 9, kernel_size=(2,), stride=(2,))\n",
       "      (3): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c0b9f1d6-05f4-40bd-ae21-63e0d4dda41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:   5%|███▎                                                              | 1/20 [00:02<00:51,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'loss': 0.979896605014801, 'mae': 0.7438562512397766, 'mse': 0.8789100646972656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  10%|██████▌                                                           | 2/20 [00:05<00:45,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'loss': 1.1313165426254272, 'mae': 0.7441172003746033, 'mse': 0.8786981105804443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  15%|█████████▉                                                        | 3/20 [00:07<00:42,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'loss': 0.8968825340270996, 'mae': 0.7439208030700684, 'mse': 0.8781741261482239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  20%|█████████████▏                                                    | 4/20 [00:09<00:39,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'loss': 1.01804518699646, 'mae': 0.7440005540847778, 'mse': 0.8785840272903442}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  25%|████████████████▌                                                 | 5/20 [00:12<00:38,  2.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'loss': 0.9445888996124268, 'mae': 0.7446054816246033, 'mse': 0.8802073001861572}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  30%|███████████████████▊                                              | 6/20 [00:15<00:36,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'loss': 0.9100013971328735, 'mae': 0.7438828349113464, 'mse': 0.8787934184074402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  35%|███████████████████████                                           | 7/20 [00:18<00:34,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 7, 'loss': 0.883246660232544, 'mae': 0.7443181276321411, 'mse': 0.879409670829773}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  40%|██████████████████████████▍                                       | 8/20 [00:21<00:32,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 8, 'loss': 0.8806824684143066, 'mae': 0.7443512082099915, 'mse': 0.8790637254714966}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  45%|█████████████████████████████▋                                    | 9/20 [00:24<00:31,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 9, 'loss': 0.8798402547836304, 'mae': 0.7443600296974182, 'mse': 0.8793436884880066}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  50%|████████████████████████████████▌                                | 10/20 [00:27<00:28,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 10, 'loss': 0.8797106742858887, 'mae': 0.7441594004631042, 'mse': 0.879389762878418}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  55%|███████████████████████████████████▊                             | 11/20 [00:30<00:26,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 11, 'loss': 0.8789178133010864, 'mae': 0.744015634059906, 'mse': 0.8786454796791077}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  60%|███████████████████████████████████████                          | 12/20 [00:33<00:23,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 12, 'loss': 0.8802182674407959, 'mae': 0.7443751096725464, 'mse': 0.8799899816513062}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  65%|██████████████████████████████████████████▎                      | 13/20 [00:36<00:21,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 13, 'loss': 0.8785892724990845, 'mae': 0.7438998222351074, 'mse': 0.8783910870552063}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  70%|█████████████████████████████████████████████▌                   | 14/20 [00:39<00:17,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 14, 'loss': 0.8800025582313538, 'mae': 0.7439092993736267, 'mse': 0.8798262476921082}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  75%|████████████████████████████████████████████████▊                | 15/20 [00:42<00:14,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 15, 'loss': 0.8803674578666687, 'mae': 0.7442479729652405, 'mse': 0.8802090287208557}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  80%|████████████████████████████████████████████████████             | 16/20 [00:45<00:11,  3.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 16, 'loss': 0.8799370527267456, 'mae': 0.7440146207809448, 'mse': 0.8797913193702698}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  85%|███████████████████████████████████████████████████████▎         | 17/20 [00:48<00:09,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 17, 'loss': 0.8806420564651489, 'mae': 0.743877112865448, 'mse': 0.8805060982704163}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  90%|██████████████████████████████████████████████████████████▌      | 18/20 [00:50<00:05,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 18, 'loss': 0.8790731430053711, 'mae': 0.7436665892601013, 'mse': 0.8789464235305786}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...:  95%|█████████████████████████████████████████████████████████████▊   | 19/20 [00:53<00:02,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 19, 'loss': 0.8791211247444153, 'mae': 0.7438399791717529, 'mse': 0.879004180431366}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...: 100%|█████████████████████████████████████████████████████████████████| 20/20 [00:56<00:00,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 20, 'loss': 0.879841685295105, 'mae': 0.743800699710846, 'mse': 0.8797286152839661}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r.fit(epochs=20, metrics=['mae','mse'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "7b57ee74-246c-450a-9f08-e6edd5b1ae32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:00<00:00, 26.24it/s]\n"
     ]
    }
   ],
   "source": [
    "rep = r.latent_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "8b52ebea-d489-499b-9441-e00c3936a16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 1000)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90680c14-da22-48ad-94c6-bf2e4d85a996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
