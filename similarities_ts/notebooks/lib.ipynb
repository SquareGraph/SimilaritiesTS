{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc8a4d8-e682-4765-b140-277eaa795ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcindabrowski/conda/envs/3.11-research/lib/python3.11/site-packages/quantining/base/engine.py:15: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import quantining.base as qt\n",
    "import torch\n",
    "import faiss\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dad5633-a008-453e-9a77-ce1faeea73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Literal\n",
    "\n",
    "class BracketAccess(type):\n",
    "    def __getitem__(cls, key: str):\n",
    "        return getattr(cls, key, None)\n",
    "\n",
    "class SampleMethods(metaclass=BracketAccess):\n",
    "\n",
    "    @staticmethod\n",
    "    def noise(scale: float, num_time_steps: int, **kwargs) -> np.ndarray:\n",
    "        return np.random.normal(scale=scale, size=num_time_steps)\n",
    "\n",
    "    @staticmethod\n",
    "    def brownian_motion(num_time_steps: int, initial_value: int, drift=0.0, \n",
    "                                volatility=1.0, \n",
    "                                dt=1.0, **kwargs):\n",
    "                                    \n",
    "        increments = np.random.normal(loc=drift*dt, scale=volatility*np.sqrt(dt), size=num_time_steps)\n",
    "    \n",
    "        # Generate forward Brownian motion path\n",
    "        path = np.cumsum(increments) + initial_value\n",
    "    \n",
    "        return path\n",
    "\n",
    "    @staticmethod\n",
    "    def random_oscillator(uniform_range: Tuple[int], num_time_steps: int, **kwargs) -> np.ndarray:\n",
    "        return np.cos(np.random.uniform(*uniform_range, num_time_steps))\n",
    "\n",
    "    @staticmethod\n",
    "    def standardize(input_array: np.ndarray) -> np.ndarray:\n",
    "        return (input_array - input_array.mean())/input_array.std()\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(input_array: np.ndarray) -> np.ndarray:\n",
    "        return (input_array - input_array.min())/(input_array.max()-input_array.min())\n",
    "\n",
    "    @staticmethod\n",
    "    def random_dataset(n_series: int, \n",
    "                       series_types: List[Literal['noise', 'brownian_motion', 'random_oscillator']], \n",
    "                       **kwargs) -> pd.DataFrame:\n",
    "\n",
    "        assert len(series_types) == n_series, AssertionError('len(series_types) must be equal to n_series')\n",
    "        for name in series_types:\n",
    "            assert name in ['noise', 'brownian_motion', 'random_oscillator'], AssertionError(f\"{name} must be one of ['noise', 'brownian_motion', 'ranom_oscillator']\")\n",
    "        \n",
    "        dataset = {}\n",
    "                           \n",
    "        for a, name in enumerate(series_types):\n",
    "            data = SampleMethods[name](**kwargs)\n",
    "\n",
    "            if 'transform' in kwargs.keys():\n",
    "                match kwargs['transform']:\n",
    "                    case 'normalize':\n",
    "                        data = SampleMethods.normalize(data)\n",
    "                    case 'standardize':\n",
    "                        data = SampleMethods.standardize(data)\n",
    "                    case other:\n",
    "                        pass\n",
    "                        \n",
    "            dataset[f\"{name}_{a}\"] = data\n",
    "\n",
    "        return pd.DataFrame(dataset)\n",
    "\n",
    "    @staticmethod\n",
    "    def all():\n",
    "        return [key for key in SampleMethods.__dict__.keys() if not key.startswith('_')][:-1]\n",
    "\n",
    "                           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b8ee24-69e4-4ec5-bd52-475b10654057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowTransform(metaclass=BracketAccess):\n",
    "\n",
    "    @staticmethod\n",
    "    def sliding_window(df: pd.DataFrame, window_length: int):\n",
    "        sw = np.squeeze(np.lib.stride_tricks.sliding_window_view(df.values, (window_length,df.shape[-1])))\n",
    "        return np.swapaxes(sw, 1,-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def non_overlapping_window(df: pd.DataFrame, window_length: int):\n",
    "        idxs = np.arange(df.shape[0]//window_length) * window_length\n",
    "        stacked = np.dstack([df.iloc[idx:idx+window_length].values for idx in idxs])\n",
    "        return np.swapaxes(stacked, 0,-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def all():\n",
    "        return [key for key in WindowTransform.__dict__.keys() if not key.startswith('_')][:-1]\n",
    "\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b98738-7711-4bcb-a4c5-12470e69cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c397a128-b327-4a58-9014-db108b4b4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class MultivariateDataset(Dataset):\n",
    "    \n",
    "    def __new__(cls, \n",
    "                mode: Literal['sliding_window', 'non_overlapping_window'], \n",
    "                df: pd.DataFrame,\n",
    "                window_length: int, \n",
    "                transform=None):\n",
    "        assert mode in WindowTransform.all(), AssertionError(f'mode must be on of {WindowTransform.methods()}')\n",
    "\n",
    "        return super().__new__(cls)\n",
    "        \n",
    "    def __init__(self, \n",
    "                 mode: Literal['sliding_window', 'non_overlapping_window'], \n",
    "                 df: pd.DataFrame, \n",
    "                 window_length: int, \n",
    "                 transform=None):\n",
    "                     \n",
    "        global DEVICE\n",
    "                     \n",
    "        self.tensors = torch.FloatTensor(WindowTransform[mode](df, window_length)).to(DEVICE)  \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.tensors[index]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transofrm(sample)\n",
    "\n",
    "        return sample #.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85463067-a17c-4f0b-95aa-1d392d5b2e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = SampleMethods.random_dataset(9, ['random_oscillator','random_oscillator','random_oscillator','brownian_motion','brownian_motion','brownian_motion','noise','noise','noise'], uniform_range=(0,1), num_time_steps=830*2880, initial_value=100, scale=0.5, transform='standardize')\n",
    "SAMPLE_DATASET = MultivariateDataset('non_overlapping_window', DF, 2880)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba2ff61-c9f0-4b9d-a880-92c1db64b713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.modules.transformer import MultiheadAttention\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, conv_filters, conv_kernel_size, conv_strides,\n",
    "                 attention_heads, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        # Initialize lists for convolutional layers\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "\n",
    "        # Add convolutional layers\n",
    "        for i in range(len(conv_filters)):\n",
    "            self.add_conv_layer(input_dim if i==0 else conv_filters[i-1],\n",
    "                                conv_filters[i], conv_kernel_size[i], conv_strides[i])\n",
    "\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.self_attention = MultiheadAttention(embed_dim=conv_filters[-1], num_heads=attention_heads)\n",
    "\n",
    "        # Fully connected layers to output the mean and standard deviation vectors\n",
    "        self.fc_mu = nn.Linear(conv_filters[-1], latent_dim)\n",
    "        self.fc_logvar = nn.Linear(conv_filters[-1], latent_dim)\n",
    "\n",
    "\n",
    "    def add_conv_layer(self, in_channels, out_channels, kernel_size, stride):\n",
    "        # Function to add a Convolutional layer followed by a ReLU activation\n",
    "        self.conv_layers.append(nn.Conv1d(in_channels, out_channels, kernel_size, stride))\n",
    "        self.conv_layers.append(nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Pass through each Convolutional layer\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # Store the output shape of the last Convolutional layer\n",
    "        self.last_conv_output_shape = x.shape\n",
    "\n",
    "        # Reshape x to match what the multi-head attention layer expects\n",
    "        x = x.permute(2, 0, 1)  # shape becomes (L, N, E)\n",
    "\n",
    "        # Apply self-attention\n",
    "        x, _ = self.self_attention(x, x, x)\n",
    "\n",
    "        # Fully connected layers to output the mean and standard deviation vectors\n",
    "        x = x.mean(dim=0)\n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        # Function to generate a random sample from the distribution defined by mu and logvar\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, conv_transpose_filters, conv_transpose_kernel_sizes, conv_transpose_strides, upsample):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Upsample layer\n",
    "        self.upsample = nn.Upsample(scale_factor=upsample)  # adjust this value as needed\n",
    "\n",
    "        # Initialize list for Convolutional Transpose layers\n",
    "        self.conv_transpose_layers = nn.ModuleList()\n",
    "\n",
    "        # Add Convolutional Transpose layers\n",
    "        for i in range(len(conv_transpose_filters)):\n",
    "            self.add_conv_transpose_layer(hidden_dim if i==0 else conv_transpose_filters[i-1],\n",
    "                                          conv_transpose_filters[i], conv_transpose_kernel_sizes[i], conv_transpose_strides[i])\n",
    "\n",
    "    def add_conv_transpose_layer(self, in_channels, out_channels, kernel_size, stride):\n",
    "        # Function to add a Convolutional Transpose layer followed by a ReLU activation\n",
    "        self.conv_transpose_layers.append(nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride))\n",
    "        self.conv_transpose_layers.append(nn.LeakyReLU())\n",
    "\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Fully connected layer\n",
    "        z = F.relu(self.fc(z))\n",
    "\n",
    "        # Reshape to 3D tensor \n",
    "        z = z.view(-1, self.hidden_dim, 1)\n",
    "\n",
    "        # Upsample\n",
    "        z = self.upsample(z)\n",
    "\n",
    "        # Pass through each Convolutional Transpose layer\n",
    "        for layer in self.conv_transpose_layers:\n",
    "            z = layer(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "class HybridVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Inspired by... with some additional changes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, latent_dim, encoder_params, decoder_params):\n",
    "        super(HybridVAE, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(input_dim, *encoder_params, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, *decoder_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.encoder.reparameterize(mu, logvar)\n",
    "        \n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "class Metrics(metaclass=BracketAccess):\n",
    "\n",
    "    @staticmethod\n",
    "    def mae(y_true, y_pred, **kwargs):\n",
    "        return torch.mean(torch.abs(y_true-y_pred))\n",
    "\n",
    "    @staticmethod\n",
    "    def mse(y_true, y_pred, **kwargs):\n",
    "        return torch.mean((y_true-y_pred)**2)\n",
    "\n",
    "    @staticmethod\n",
    "    def mase(y_true, y_pred, y_naive, **kwargs):\n",
    "        mae = Metrics.mae(y_true, y_pred)\n",
    "        scale = Metrics.mae(y_true, y_naive)\n",
    "        return mae/scale\n",
    "\n",
    "    def all():\n",
    "        return [key for key in Metrics.__dict__.keys() if not key.startswith(\"_\")][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d725cd-2020-4eb6-9dc6-9e73f7fff6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "Optimizers = Enum(\"Optimizers\", {\n",
    " 'Adadelta': torch.optim.Adadelta,\n",
    " 'Adagrad': torch.optim.Adagrad,\n",
    " 'Adam': torch.optim.Adam,\n",
    " 'AdamW': torch.optim.AdamW,\n",
    " 'SparseAdam': torch.optim.SparseAdam,\n",
    " 'Adamax': torch.optim.Adamax,\n",
    " 'ASGD': torch.optim.ASGD,\n",
    " 'SGD': torch.optim.SGD,\n",
    " 'RAdam': torch.optim.RAdam,\n",
    " 'Rprop': torch.optim.Rprop,\n",
    " 'RMSprop': torch.optim.RMSprop,\n",
    " 'Optimizer': torch.optim.Optimizer,\n",
    " 'NAdam': torch.optim.NAdam,\n",
    " 'LBFGS': torch.optim.LBFGS,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "123c740f-f413-4d43-84e4-3d8667c50155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Sequence\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "class Reducer:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __new__(cls, \n",
    "                dataset: MultivariateDataset, \n",
    "                batch_size: int,\n",
    "                optimizer: Literal['Adadelta','Adagrad','Adam','AdamW','SparseAdam','Adamax','ASGD','SGD','RAdam','Rprop','RMSprop','Optimizer','NAdam','LBFGS'],\n",
    "                latent_dim: int, \n",
    "                conv_filters: Sequence[int], \n",
    "                conv_kernel_size: Sequence[int], \n",
    "                conv_strides: Sequence[int], \n",
    "                attention_heads: int,\n",
    "                hidden_dim: int, \n",
    "                conv_transpose_filters: Sequence[int], \n",
    "                conv_transpose_kernel_sizes:Sequence[int], \n",
    "                conv_transpose_strides:Sequence[int], \n",
    "                upsample: int):\n",
    "\n",
    "        \n",
    "        assert len(conv_filters) == len(conv_kernel_size) == len(conv_strides), AssertionError(\"All encoder arguments have to have same length\")\n",
    "        assert isinstance(dataset, MultivariateDataset), AssertionError(\"Dataset have to be of type MultivariateDataset\")\n",
    "\n",
    "        return super().__new__(cls)\n",
    "                     \n",
    "    def __init__(self, \n",
    "                 dataset: MultivariateDataset, \n",
    "                 batch_size: int,\n",
    "                 optimizer: Literal['Adadelta','Adagrad','Adam','AdamW','SparseAdam','Adamax','ASGD','SGD','RAdam','Rprop','RMSprop','Optimizer','NAdam','LBFGS'],\n",
    "                 latent_dim: int, \n",
    "                 conv_filters: Sequence[int], \n",
    "                 conv_kernel_size: Sequence[int], \n",
    "                 conv_strides: Sequence[int], \n",
    "                 attention_heads: int,\n",
    "                 hidden_dim: int, \n",
    "                 conv_transpose_filters: Sequence[int], \n",
    "                 conv_transpose_kernel_sizes:Sequence[int], \n",
    "                 conv_transpose_strides:Sequence[int], \n",
    "                 upsample: int) -> None:\n",
    "    \n",
    "        global DEVICE\n",
    "\n",
    "        self.data_loader = DataLoader(dataset, batch_size=batch_size)                      \n",
    "        self.model = HybridVAE(input_dim=dataset.tensors.shape[1], \n",
    "                               latent_dim=latent_dim,\n",
    "                               encoder_params=(conv_filters, conv_kernel_size, conv_strides, attention_heads),\n",
    "                               decoder_params=(hidden_dim, conv_transpose_filters, \n",
    "                                               conv_transpose_kernel_sizes, conv_transpose_strides, \n",
    "                                               upsample)).to(DEVICE)\n",
    "        self.optimizer = Optimizers[optimizer].value(params=self.model.parameters())\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_hybrid_vae(recon_x, x, mu, logvar) -> torch.Tensor:\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(recon_x, x)\n",
    "    \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "        # Total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "    \n",
    "        return loss\n",
    "        \n",
    "    def fit(self,\n",
    "            epochs: int = 5, \n",
    "            metrics: Sequence[str] = None, schedule: bool = False,\n",
    "            **kwargs) -> None:\n",
    "\n",
    "        \"\"\"\n",
    "        Kwargs y_naive if metrics is mase\n",
    "        \"\"\"\n",
    "        \n",
    "        if metrics != None:\n",
    "            assert hasattr(metrics, \"__iter__\"), AssertionError('If not none, metrics have to be iterable object')\n",
    "            for name in metrics:\n",
    "                assert name in Metrics.all(), AssertionError(f\"{name} must be one of {Metrics.all()}.\")\n",
    "        \n",
    "    \n",
    "        desc = \"Fitting VAE model on dataset...\"\n",
    "                \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, 'min')\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in tqdm(range(epochs), total=epochs, desc=desc):\n",
    "            summary = {'epoch':epoch+1}\n",
    "            for batch in self.data_loader:\n",
    "                x = batch\n",
    "                self.optimizer.zero_grad()\n",
    "                recon_x, mu, logvar = self.model(x)\n",
    "                loss = Reducer.loss_hybrid_vae(recon_x, x, mu, logvar)\n",
    "                summary['loss'] = loss.item() #.:4f\n",
    "                if metrics is not None:\n",
    "                    for name in metrics:\n",
    "                        summary[name] = Metrics[name](recon_x, x, **kwargs).item() #:.4f\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            print(summary)\n",
    "            \n",
    "            if schedule:\n",
    "                scheduler.step(loss)\n",
    "\n",
    "    def generate(self) -> Sequence:\n",
    "        pass\n",
    "    \n",
    "    def latent_rep(self, as_numpy: bool = True) -> Sequence:\n",
    "\n",
    "        encoded = []\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.data_loader):\n",
    "                x = batch\n",
    "                mu, logvar = self.model.encoder(x)\n",
    "                z = self.model.encoder.reparameterize(mu, logvar)\n",
    "                encoded.append(z)\n",
    "\n",
    "        if as_numpy:\n",
    "            return np.vstack([item.cpu().numpy() for item in encoded])\n",
    "            \n",
    "        return encoded\n",
    "\n",
    "    def decode(latent_rep: List[torch.Tensor]) -> Sequence:\n",
    "        return [self.model.decoder(entry) for entry in latent_rep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5810d6c-7921-4ad6-8df1-ccf878734bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONV_FILTERS = [64,128]\n",
    "EXAMPLE_PARAMS = {'dataset': SAMPLE_DATASET, \n",
    "                 'batch_size': 32,\n",
    "                 'optimizer': 'Adam',\n",
    "                 'latent_dim': 1000, \n",
    "                 'conv_filters': CONV_FILTERS,\n",
    "                 'conv_kernel_size': [3,3], \n",
    "                 'conv_strides': [2,2],\n",
    "                 'attention_heads':8,\n",
    "                 'hidden_dim': 400, \n",
    "                 'conv_transpose_filters': [128,9], \n",
    "                 'conv_transpose_kernel_sizes': [2,2], \n",
    "                 'conv_transpose_strides': [2,2], \n",
    "                 'upsample': SAMPLE_DATASET.tensors.shape[-1]/(len(CONV_FILTERS)*2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9019863-63f1-48cd-b1a8-6e26b8a9399f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Reducer(**EXAMPLE_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0b9f1d6-05f4-40bd-ae21-63e0d4dda41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting VAE model on dataset...: 100%|███████████████████████████████████████████████████████████████████| 1/1 [01:27<00:00, 87.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'loss': 3.15842866897583, 'mae': 0.8731301426887512, 'mse': 1.0963951349258423}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "r.fit(epochs=1, metrics=['mae','mse'], schedule=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d9037f3-80bc-49f7-932e-df3cb92a2872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 26/26 [00:27<00:00,  1.06s/it]\n"
     ]
    }
   ],
   "source": [
    "rep = r.latent_rep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f3c1d4-decd-457a-ac8d-ca00fb2454c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(0)\n",
    "# faiss.normalize_L2(unpacked)\n",
    "# dimension = unpacked.shape[1]\n",
    "# index = faiss.IndexFlatL2(dimension)\n",
    "# res = faiss.StandardGpuResources()\n",
    "# gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "# gpu_index.add(unpacked)\n",
    "# k = 10\n",
    "# niter = 20\n",
    "# kmeans = faiss.Kmeans(dimension, k, niter=niter, gpu=True)\n",
    "# kmeans.train(unpacked)\n",
    "# centroids = kmeans.centroids\n",
    "# D, I = kmeans.index.search(unpacked, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddf51481-920a-48d9-8317-1b038d887df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusters:\n",
    "\n",
    "    def __init__(self, \n",
    "                 latent_rep: \n",
    "                 np.ndarray, \n",
    "                 seed: int, \n",
    "                 n_centroids:int, \n",
    "                 n_iter: int, \n",
    "                 gpu: bool = False):\n",
    "                     \n",
    "        np.random.seed(seed)\n",
    "        print(latent_rep[0])\n",
    "        faiss.normalize_L2(latent_rep)\n",
    "        print(latent_rep[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beba9d5-b7cf-4f8c-8d3f-0d7a916991b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Clusters(rep, 0, 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2d4748-1f34-4706-8882-d9e7538e758e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
